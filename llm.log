2024-01-19 23:03:20,007 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

END_TOKEN = "
2024-01-19 23:03:21,461 - INFO - 127.0.0.1 - - [19/Jan/2024 23:03:21] "POST / HTTP/1.1" 200 -
2024-01-19 23:04:18,939 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

END_TOKEN = "
2024-01-19 23:04:21,358 - INFO - 127.0.0.1 - - [19/Jan/2024 23:04:21] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:51,197 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)


2024-01-19 23:05:52,462 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

wi
2024-01-19 23:05:52,721 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with
2024-01-19 23:05:53,100 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with 
2024-01-19 23:05:53,315 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with o
2024-01-19 23:05:53,416 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with op
2024-01-19 23:05:53,586 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with ope
2024-01-19 23:05:53,684 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open
2024-01-19 23:05:53,869 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:53] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:54,611 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open(
2024-01-19 23:05:55,290 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:55] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:55,576 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:55] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:55,580 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:55] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:55,899 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:55] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:55,995 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("
2024-01-19 23:05:56,163 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:56] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:56,673 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:56] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:56,937 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:56] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:57,271 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:57] "POST / HTTP/1.1" 200 -
2024-01-19 23:05:58,432 - INFO - 127.0.0.1 - - [19/Jan/2024 23:05:58] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:00,512 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("c
2024-01-19 23:06:00,618 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("co
2024-01-19 23:06:00,748 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("con
2024-01-19 23:06:01,092 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf
2024-01-19 23:06:01,205 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.
2024-01-19 23:06:01,568 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.y
2024-01-19 23:06:01,832 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.ym
2024-01-19 23:06:01,937 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml
2024-01-19 23:06:02,731 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:02] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:02,828 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml",
2024-01-19 23:06:02,928 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:02] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:03,004 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", 
2024-01-19 23:06:03,440 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:03] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:03,678 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:03] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:03,759 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:03] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:03,907 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:03] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:04,187 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "
2024-01-19 23:06:04,290 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:04] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:04,349 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:04] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:04,829 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r
2024-01-19 23:06:05,577 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:05] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:05,630 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r"
2024-01-19 23:06:05,681 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:05] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:06,886 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:06] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:08,210 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:08] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:08,688 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") 
2024-01-19 23:06:09,102 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r")
2024-01-19 23:06:09,121 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:09] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:09,471 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r"):
2024-01-19 23:06:09,778 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r"): 
2024-01-19 23:06:10,202 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r"):
2024-01-19 23:06:10,365 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r")
2024-01-19 23:06:10,821 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:10] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:11,098 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:11,136 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") 
2024-01-19 23:06:11,298 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:11,478 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") a
2024-01-19 23:06:11,648 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as
2024-01-19 23:06:11,962 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as 
2024-01-19 23:06:12,244 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f
2024-01-19 23:06:12,251 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:12] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:12,618 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:12] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:12,635 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
2024-01-19 23:06:13,011 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    
2024-01-19 23:06:13,371 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:13] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:13,565 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:13] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:13,793 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:13] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:14,086 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:14] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:15,028 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:15] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:15,031 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:15] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:15,052 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:15] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:17,196 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:17] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:19,790 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    
2024-01-19 23:06:22,701 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:22] "POST / HTTP/1.1" 200 -
2024-01-19 23:06:24,938 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

chat_model = AzureChatOpenAI(config["azure_chat_openai"])

class Chat(Resource):
    def post(self):
        data = request.get_json()
        message = data["message"]
        
        # Send message to chat model
        response = chat_model.send_message(message)
        
        # Process response
        if response["role"] == "system":
            system_message = SystemMessage(response["message"])
            system_message.process()
            return system_message.to_dict()
        else:
            human_message = HumanMessage(response["message"])
            human_message.process()
            return human_message.to_dict()

api.add_resource(Chat, "/chat")

if __name__ == "__main__":
    app.run(debug=True)
2024-01-19 23:06:29,587 - INFO - 127.0.0.1 - - [19/Jan/2024 23:06:29] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:01,303 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    co
2024-01-19 23:07:01,455 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    con
2024-01-19 23:07:02,155 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    conf
2024-01-19 23:07:02,329 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    confi
2024-01-19 23:07:02,834 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config
2024-01-19 23:07:03,110 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config 
2024-01-19 23:07:03,233 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config =
2024-01-19 23:07:03,376 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = 
2024-01-19 23:07:03,526 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:03] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:03,948 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:03] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:05,310 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:05] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:05,314 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:05] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:05,316 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:05] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:05,319 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:05] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:07,503 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:07] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:07,506 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:07] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:10,398 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config =
2024-01-19 23:07:10,663 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = 
2024-01-19 23:07:12,239 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:12] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:14,007 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:14] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:17,238 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config =
2024-01-19 23:07:17,478 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = 
2024-01-19 23:07:19,891 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:19] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:19,895 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:19] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:20,991 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml
2024-01-19 23:07:21,384 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.
2024-01-19 23:07:21,694 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load
2024-01-19 23:07:21,970 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(
2024-01-19 23:07:22,457 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f
2024-01-19 23:07:22,842 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)


2024-01-19 23:07:23,797 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:23] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:23,878 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:23] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:24,772 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:24] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:25,376 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:25] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:25,380 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:25] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:25,402 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:25] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:33,778 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN 
2024-01-19 23:07:33,935 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN =
2024-01-19 23:07:34,053 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = 
2024-01-19 23:07:34,766 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:34] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:34,769 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:34] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:37,044 - INFO - 127.0.0.1 - - [19/Jan/2024 23:07:37] "POST / HTTP/1.1" 200 -
2024-01-19 23:07:58,476 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN =
2024-01-19 23:07:58,646 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = 
2024-01-19 23:08:06,255 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN =
2024-01-19 23:08:06,552 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = 
2024-01-19 23:08:10,263 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:10] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:10,281 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:10] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:10,544 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:10] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:10,664 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:10] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:13,069 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config[
2024-01-19 23:08:14,018 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["
2024-01-19 23:08:15,446 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:15] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:17,437 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:17] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:28,014 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]

2024-01-19 23:08:30,256 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:30] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:33,297 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN 
2024-01-19 23:08:33,431 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN =
2024-01-19 23:08:33,552 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = 
2024-01-19 23:08:34,788 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = co
2024-01-19 23:08:34,930 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = con
2024-01-19 23:08:35,599 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:35] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:36,028 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:36] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:36,163 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:36] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:36,364 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = con
2024-01-19 23:08:36,681 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config
2024-01-19 23:08:37,275 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:37] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:37,380 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config[
2024-01-19 23:08:37,563 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:37] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:38,126 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:38] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:38,676 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["
2024-01-19 23:08:39,714 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:39] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:40,325 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:40] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:41,415 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:41] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:43,989 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]

2024-01-19 23:08:47,394 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:47] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:47,604 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN 
2024-01-19 23:08:47,741 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN =
2024-01-19 23:08:47,862 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = 
2024-01-19 23:08:48,557 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:48] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:48,615 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:48] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:48,951 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = co
2024-01-19 23:08:49,090 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = con
2024-01-19 23:08:50,772 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:50] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:50,779 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config
2024-01-19 23:08:51,155 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["
2024-01-19 23:08:51,378 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:51] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:51,465 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN
2024-01-19 23:08:51,600 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:51] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:52,833 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:52] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:52,987 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
2024-01-19 23:08:53,853 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
END_TOKEN = "
2024-01-19 23:08:54,168 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:54] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:54,308 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:54] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:56,007 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:56] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:56,453 - INFO - 127.0.0.1 - - [19/Jan/2024 23:08:56] "POST / HTTP/1.1" 200 -
2024-01-19 23:08:57,376 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
2024-01-19 23:08:58,381 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:08:59,284 - WARNING -  * Debugger is active!
2024-01-19 23:08:59,298 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:09:04,353 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:09:05,028 - WARNING -  * Debugger is active!
2024-01-19 23:09:05,040 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:09:12,651 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"] 
2024-01-19 23:09:15,680 - INFO - 127.0.0.1 - - [19/Jan/2024 23:09:15] "POST / HTTP/1.1" 200 -
2024-01-19 23:09:33,387 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:09:34,067 - WARNING -  * Debugger is active!
2024-01-19 23:09:34,080 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:09:39,145 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:09:39,833 - WARNING -  * Debugger is active!
2024-01-19 23:09:39,846 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:09:41,874 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:09:42,562 - WARNING -  * Debugger is active!
2024-01-19 23:09:42,575 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:09:45,614 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
2024-01-19 23:09:46,638 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:09:47,335 - WARNING -  * Debugger is active!
2024-01-19 23:09:47,348 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:10:19,333 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context))
2024-01-19 23:10:20,504 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context),
2024-01-19 23:10:20,663 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context), 
2024-01-19 23:10:21,750 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context), "
2024-01-19 23:10:22,740 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context), "g
2024-01-19 23:10:23,087 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context), "gr
2024-01-19 23:10:23,371 - INFO - 127.0.0.1 - - [19/Jan/2024 23:10:23] "POST / HTTP/1.1" 200 -
2024-01-19 23:10:23,470 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context), "gre
2024-01-19 23:10:23,594 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context), "gree
2024-01-19 23:10:23,811 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]

class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Make code completion after the end token.
        \n\n
        {fore_context} 
        """ 
        logging.info(colored(fore_context), "green
2024-01-19 23:10:24,416 - INFO - 127.0.0.1 - - [19/Jan/2024 23:10:24] "POST / HTTP/1.1" 200 -
2024-01-19 23:10:24,737 - INFO - 127.0.0.1 - - [19/Jan/2024 23:10:24] "POST / HTTP/1.1" 200 -
2024-01-19 23:10:25,015 - INFO - 127.0.0.1 - - [19/Jan/2024 23:10:25] "POST / HTTP/1.1" 200 -
2024-01-19 23:10:25,743 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:10:26,594 - WARNING -  * Debugger is active!
2024-01-19 23:10:26,607 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:10:45,652 - INFO - 127.0.0.1 - - [19/Jan/2024 23:10:45] "POST / HTTP/1.1" 200 -
2024-01-19 23:10:46,301 - INFO - 127.0.0.1 - - [19/Jan/2024 23:10:46] "POST / HTTP/1.1" 200 -
2024-01-19 23:10:46,825 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:10:47,661 - WARNING -  * Debugger is active!
2024-01-19 23:10:47,673 - INFO -  * Debugger PIN: 253-101-103
