2024-01-19 23:23:32,673 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]
2024-01-19 23:23:32,938 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]

2024-01-19 23:23:35,081 - INFO - 127.0.0.1 - - [19/Jan/2024 23:23:35] "POST / HTTP/1.1" 200 -
2024-01-19 23:23:35,159 - INFO - 127.0.0.1 - - [19/Jan/2024 23:23:35] "POST / HTTP/1.1" 200 -
2024-01-19 23:23:53,808 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:23:53,923 - INFO -  * Restarting with stat
2024-01-19 23:23:54,661 - WARNING -  * Debugger is active!
2024-01-19 23:23:54,674 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:23:58,037 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
            
2024-01-19 23:24:01,627 - INFO - 127.0.0.1 - - [19/Jan/2024 23:24:01] "POST / HTTP/1.1" 200 -
2024-01-19 23:24:03,798 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        
2024-01-19 23:24:04,092 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
    
2024-01-19 23:24:04,314 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,

2024-01-19 23:24:04,675 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
2024-01-19 23:24:05,812 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:24:05,920 - INFO -  * Restarting with stat
2024-01-19 23:24:06,515 - WARNING -  * Debugger is active!
2024-01-19 23:24:06,528 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:24:07,763 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        
2024-01-19 23:24:08,955 - INFO - 127.0.0.1 - - [19/Jan/2024 23:24:08] "POST / HTTP/1.1" 200 -
2024-01-19 23:24:12,417 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        
2024-01-19 23:24:13,591 - INFO - 127.0.0.1 - - [19/Jan/2024 23:24:13] "POST / HTTP/1.1" 200 -
2024-01-19 23:24:23,312 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """

2024-01-19 23:24:24,650 - INFO - 127.0.0.1 - - [19/Jan/2024 23:24:24] "POST / HTTP/1.1" 200 -
2024-01-19 23:24:24,738 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:24:24,848 - INFO -  * Restarting with stat
2024-01-19 23:24:25,421 - WARNING -  * Debugger is active!
2024-01-19 23:24:25,434 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:24:26,314 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        
2024-01-19 23:24:29,787 - INFO - 127.0.0.1 - - [19/Jan/2024 23:24:29] "POST / HTTP/1.1" 200 -
2024-01-19 23:24:31,845 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        
2024-01-19 23:24:34,371 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 

2024-01-19 23:24:35,168 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
2024-01-19 23:24:35,219 - INFO - 127.0.0.1 - - [19/Jan/2024 23:24:35] "POST / HTTP/1.1" 200 -
2024-01-19 23:24:35,551 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:24:35,661 - INFO -  * Restarting with stat
2024-01-19 23:24:36,262 - WARNING -  * Debugger is active!
2024-01-19 23:24:36,275 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:24:37,540 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        
2024-01-19 23:24:38,794 - INFO - 127.0.0.1 - - [19/Jan/2024 23:24:38] "POST / HTTP/1.1" 200 -
2024-01-19 23:24:40,179 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
    
2024-01-19 23:24:40,467 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)

2024-01-19 23:24:40,791 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
2024-01-19 23:24:41,336 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:24:41,451 - INFO -  * Restarting with stat
2024-01-19 23:24:42,033 - WARNING -  * Debugger is active!
2024-01-19 23:24:42,046 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:24:45,063 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

2024-01-19 23:24:47,102 - INFO - 127.0.0.1 - - [19/Jan/2024 23:24:47] "POST / HTTP/1.1" 200 -
2024-01-19 23:25:06,774 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

2024-01-19 23:25:09,379 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:25:09,494 - INFO -  * Restarting with stat
2024-01-19 23:25:10,074 - WARNING -  * Debugger is active!
2024-01-19 23:25:10,087 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:25:11,738 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
            # Returning a response with status code 200
            return {'generated_text': llm.complete_code(inputs)}, 200
        else:
            return "Content type is not supported."


2024-01-19 23:25:12,059 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
            # Returning a response with status code 200
            return {'generated_text': llm.complete_code(inputs)}, 200
        else:
            return "Content type is not supported."

2024-01-19 23:25:14,101 - INFO - 127.0.0.1 - - [19/Jan/2024 23:25:14] "POST / HTTP/1.1" 200 -
2024-01-19 23:25:15,957 - INFO - 127.0.0.1 - - [19/Jan/2024 23:25:15] "POST / HTTP/1.1" 200 -
2024-01-19 23:25:46,333 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
            # Returning a response with status code 200
            return {'generated_text': llm.complete_code(inputs)}, 200
        else:
            return "Content type is not supported."

2024-01-19 23:25:49,565 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:25:49,673 - INFO -  * Restarting with stat
2024-01-19 23:25:50,241 - WARNING -  * Debugger is active!
2024-01-19 23:25:50,254 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:26:29,951 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-01-19 23:26:29,951 - INFO - [33mPress CTRL+C to quit[0m
2024-01-19 23:26:29,952 - INFO -  * Restarting with stat
2024-01-19 23:26:30,519 - WARNING -  * Debugger is active!
2024-01-19 23:26:30,533 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:26:45,698 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:26:45,815 - INFO -  * Restarting with stat
2024-01-19 23:26:46,390 - WARNING -  * Debugger is active!
2024-01-19 23:26:46,402 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:27:27,859 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:27:27,976 - INFO -  * Restarting with stat
2024-01-19 23:27:28,711 - WARNING -  * Debugger is active!
2024-01-19 23:27:28,724 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:28:27,468 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
            # Returning a response with status code 200
            return {'generated_text': llm.complete_code(inputs)}, 200
        else:
            return "Content type is not supported."
2024-01-19 23:28:27,746 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
            # Returning a response with status code 200
            return {'generated_text': llm.complete_code(inputs)}, 200
        else:
            return "Content type is not supported."
        
2024-01-19 23:28:28,527 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
            # Returning a response with status code 200
            return {'generated_text': llm.complete_code(inputs)}, 200
        else:
            return "Content type is not supported."
    
2024-01-19 23:28:28,738 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:28] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:28,921 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
            # Returning a response with status code 200
            return {'generated_text': llm.complete_code(inputs)}, 200
        else:
            return "Content type is not supported."

2024-01-19 23:28:28,957 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:28] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:29,726 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:29] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:30,033 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:30] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:36,909 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
            
2024-01-19 23:28:38,239 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:38] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:40,075 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
        
2024-01-19 23:28:40,416 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
    
2024-01-19 23:28:40,692 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']

2024-01-19 23:28:41,063 - INFO - from flask import Flask, request
from flask_restful import reqparse, abort, Api, Resource
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import yaml
import json
from pprint import pprint
from termcolor import colored

import logging

# Configure the logging module
logging.basicConfig(filename='llm.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)
api = Api(app)

with open("conf.yml", "r") as f:
    config = yaml.safe_load(f)

END_TOKEN = config["END_TOKEN"]
START_TOKEN = config["START_TOKEN"]
MID_TOKEN = config["MID_TOKEN"]
MAX_TOKENS = config["MAX_TOKENS"]


class LLM:
    verbose = False
    def __init__(self, conf_file) -> None:
        with open(conf_file, "r", encoding="utf-8") as f:
            conf = yaml.safe_load(f.read())
        OPENAI_API_KEY = conf["OPENAI_API_KEY"]
        OPENAI_API_BASE = conf["OPENAI_API_BASE"]
        OPENAI_API_VERSION = conf["OPENAI_API_VERSION"]
        DEPLOYMENT = conf["DEPLOYMENT"]
        self.llm = AzureChatOpenAI(
            openai_api_type="azure",
            openai_api_version=OPENAI_API_VERSION,
            openai_api_base=OPENAI_API_BASE,
            openai_api_key=OPENAI_API_KEY,
            deployment_name=DEPLOYMENT,
            temperature=0,
            max_tokens=MAX_TOKENS,
        )

    def complete_code(self, code_context):
        """Take the input from the request and output.

        args:
            code_context(str): the code_context

        return(dict): the response
        """
        fore_context = get_fore_context(code_context)
        system_setting = SystemMessage(
            content="You are a code autocompleter.")
        prompt = f"""
        Please complete code for the following code. Only output code text without markdown. Make code completion after the end token.
        \n\n
        {fore_context}
        """ 
        logging.info(fore_context)
        message = HumanMessage(content=prompt)
        return self.llm.invoke([system_setting, message]).content

llm = LLM(".env-35-16k.yml")

def get_fore_context(inputs):
    return inputs[: inputs.find(END_TOKEN)].replace(START_TOKEN,"") 

class Health(Resource):
    def get(self):
        return {'message': 'healthy'}, 200

class Generate(Resource):
    def post(self):
        content_type = request.headers.get('Content-Type')
        if (content_type == 'application/json'):
            request_json = request.get_json()
            inputs = request_json['inputs']
2024-01-19 23:28:41,292 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:41] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:41,621 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:41] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:41,973 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:41] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:42,397 - INFO - 127.0.0.1 - - [19/Jan/2024 23:28:42] "POST / HTTP/1.1" 200 -
2024-01-19 23:28:42,487 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:28:42,601 - INFO -  * Restarting with stat
2024-01-19 23:28:43,371 - WARNING -  * Debugger is active!
2024-01-19 23:28:43,385 - INFO -  * Debugger PIN: 253-101-103
2024-01-19 23:29:07,966 - INFO - import spacy
import spacy

2024-01-19 23:29:08,382 - INFO - import spacy
import spacy
2024-01-19 23:29:09,122 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:09] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:09,405 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:09] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:09,488 - INFO - import spacy
import spac
2024-01-19 23:29:09,696 - INFO - import spacy
import spa
2024-01-19 23:29:09,853 - INFO - import spacy
import sp
2024-01-19 23:29:09,982 - INFO - import spacy
import s
2024-01-19 23:29:10,117 - INFO - import spacy
import 
2024-01-19 23:29:10,253 - INFO - import spacy
import
2024-01-19 23:29:10,375 - INFO - import spacy
impor
2024-01-19 23:29:10,442 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:10] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:10,505 - INFO - import spacy
impo
2024-01-19 23:29:10,635 - INFO - import spacy
imp
2024-01-19 23:29:10,748 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:10] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:10,780 - INFO - import spacy
im
2024-01-19 23:29:10,786 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:10] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:10,916 - INFO - import spacy
i
2024-01-19 23:29:11,030 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:11,076 - INFO - import spacy

2024-01-19 23:29:11,112 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:11,129 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:11,333 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:11,592 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:11,746 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:11,891 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:11,998 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:11] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:12,148 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:12] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:13,173 - INFO - import spacy

2024-01-19 23:29:14,376 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:14] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:20,972 - INFO - import spacy
import spacy

nlp = spacy.load("en_core_web_sm")

text = "Enter your text here"

doc = nlp(text)


2024-01-19 23:29:22,940 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:22] "POST / HTTP/1.1" 200 -
2024-01-19 23:29:35,205 - INFO - import spacy
import spacy

nlp = spacy.load("en_core_web_sm")

text = "Enter your text here"

doc = nlp(text)

import spacy

nlp = spacy.load("en_core_web_sm")

text = "Enter your text here"

doc = nlp(text)


2024-01-19 23:29:36,937 - INFO - 127.0.0.1 - - [19/Jan/2024 23:29:36] "POST / HTTP/1.1" 200 -
2024-01-19 23:32:39,959 - INFO -  * Detected change in '/Users/haopengwu/MetaSpace/repos/llm-vscode-server/main.py', reloading
2024-01-19 23:32:40,075 - INFO -  * Restarting with stat
2024-01-19 23:32:40,773 - WARNING -  * Debugger is active!
2024-01-19 23:32:40,786 - INFO -  * Debugger PIN: 253-101-103
