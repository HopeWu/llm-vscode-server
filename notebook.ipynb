{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fore_context(inputs):\n",
    "    return inputs[: inputs.find(\"{end token}\")].replace(\"{start token}\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, conf_file) -> None:\n",
    "        with open(conf_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            conf = yaml.safe_load(f.read())\n",
    "        OPENAI_API_KEY = conf[\"OPENAI_API_KEY\"]\n",
    "        OPENAI_API_BASE = conf[\"OPENAI_API_BASE\"]\n",
    "        OPENAI_API_VERSION = conf[\"OPENAI_API_VERSION\"]\n",
    "        DEPLOYMENT = conf[\"DEPLOYMENT\"]\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            openai_api_type=\"azure\",\n",
    "            openai_api_version=OPENAI_API_VERSION,\n",
    "            openai_api_base=OPENAI_API_BASE,\n",
    "            openai_api_key=OPENAI_API_KEY,\n",
    "            deployment_name=DEPLOYMENT,\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "    def complete(self, query):\n",
    "        \"\"\"Answer to a query.\"\"\"\n",
    "        system_setting = SystemMessage(\n",
    "            content=\"You help to complete the user's code.\")\n",
    "        \n",
    "        message = HumanMessage(content=query)\n",
    "        return self.llm.invoke([system_setting, message]).content\n",
    "    \n",
    "    def complete_code(self, code_context):\n",
    "        \"\"\"Take the input from the request and output.\n",
    "\n",
    "        args:\n",
    "            code_context(str): the code_context\n",
    "\n",
    "        return(dict): the response\n",
    "        \"\"\"\n",
    "        fore_context = get_fore_context(code_context)\n",
    "        system_setting = SystemMessage(\n",
    "            content=\"You are a code autocompleter.\")\n",
    "        prompt = f\"\"\"\n",
    "        Please complete code for the following code. Make code completion after the end token.\n",
    "        \\n\\n\n",
    "        {code_context}\n",
    "        \"\"\"\n",
    "        message = HumanMessage(content=prompt)\n",
    "        return self.llm.invoke([system_setting, message]).content\n",
    "\n",
    "llm = LLM(\".env-35-16k.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = '{start token}import numpy as np\\nimport scipy as sp\\n{end token}def hello_world():\\n    print(\"Hello world\"){middle token}'\n",
    "data = {\"inputs\": inputs, \"parameters\": {\"max_new_tokens\": 256}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport scipy as sp\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fore_context(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here is the completed code:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'import numpy as np\\n'\n",
      " 'import scipy as sp\\n'\n",
      " '\\n'\n",
      " 'def hello_world():\\n'\n",
      " '    print(\"Hello world\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'Please let me know if you need any further assistance.')\n"
     ]
    }
   ],
   "source": [
    "pprint(llm.complete_code(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "false = False\n",
    "generate_response = {\n",
    "  \"details\": {\n",
    "    \"best_of_sequences\": [\n",
    "      {\n",
    "        \"finish_reason\": \"length\",\n",
    "        \"generated_text\": \"test\",\n",
    "        \"generated_tokens\": 1,\n",
    "        \"prefill\": [\n",
    "          {\n",
    "            \"id\": 0,\n",
    "            \"logprob\": -0.34,\n",
    "            \"text\": \"test\"\n",
    "          }\n",
    "        ],\n",
    "        \"seed\": 42,\n",
    "        \"tokens\": [\n",
    "          {\n",
    "            \"id\": 0,\n",
    "            \"logprob\": -0.34,\n",
    "            \"special\": false,\n",
    "            \"text\": \"test\"\n",
    "          }\n",
    "        ],\n",
    "        \"top_tokens\": [\n",
    "          [\n",
    "            {\n",
    "              \"id\": 0,\n",
    "              \"logprob\": -0.34,\n",
    "              \"special\": false,\n",
    "              \"text\": \"test\"\n",
    "            }\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"finish_reason\": \"length\",\n",
    "    \"generated_tokens\": 1,\n",
    "    \"prefill\": [\n",
    "      {\n",
    "        \"id\": 0,\n",
    "        \"logprob\": -0.34,\n",
    "        \"text\": \"test\"\n",
    "      }\n",
    "    ],\n",
    "    \"seed\": 42,\n",
    "    \"tokens\": [\n",
    "      {\n",
    "        \"id\": 0,\n",
    "        \"logprob\": -0.34,\n",
    "        \"special\": false,\n",
    "        \"text\": \"test\"\n",
    "      }\n",
    "    ],\n",
    "    \"top_tokens\": [\n",
    "      [\n",
    "        {\n",
    "          \"id\": 0,\n",
    "          \"logprob\": -0.34,\n",
    "          \"special\": false,\n",
    "          \"text\": \"test\"\n",
    "        }\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"generated_text\": \"test\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['details', 'generated_text'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask-restful-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
